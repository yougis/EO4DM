{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook pour le calcul des corrélations croisées (servant à l'évaluation/comparaison des indicateurs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation contexte test appli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WRK_DIR = os.path.normpath('D:/MATHIS/0_Projet_Secheresse/1_Scripts/toolbox/eo4dm-oeil/EO4DM')\n",
    "os.chdir(WRK_DIR)\n",
    "WRK_DIR = os.path.join(WRK_DIR,'..')\n",
    "\n",
    "TERRITORY = 'New Caledonia (Fr)'\n",
    "PRODUCT_OBS = 'VHI'\n",
    "PRODUCT_REF = 'SPI_ref_1991_2020'\n",
    "PERIOD_OBS = 'M'  # if no period, set ''\n",
    "PERIOD_REF = ''   # if no period, set ''\n",
    "LANDMASK_OBS = 'NoTrees_NoBuild'  # if no mask, set ''\n",
    "LANDMASK_REF = ''  # if no mask, set ''\n",
    "\n",
    "TERRITORY_str = TERRITORY.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "# DATA_HISTO = os.path.join(WRK_DIR,'DATA_HISTO',TERRITORY_str)\n",
    "DATA_HISTO = os.path.join(WRK_DIR,'DMPIPELINE_WORKST/DATA_HISTO_backup',TERRITORY_str)\n",
    "# ANNEX_DIR = os.path.join(WRK_DIR,'ANNEX',TERRITORY_str)\n",
    "ANNEX_DIR= os.path.join(WRK_DIR,'DMPIPELINE_WORKST/ANNEX',TERRITORY_str)\n",
    "INDIR_STATS_OBS = os.path.join(DATA_HISTO,'1_INDICATEURS/GLOBAL/STATS')\n",
    "INDIR_STATS_REF = os.path.join(DATA_HISTO,'1_INDICATEURS/ALERT/METEO')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dmpipeline.ALERT_Processing.ALERT_main_functions as alertmain\n",
    "import dmpipeline.GEOSTATS_Processing.GEOSTATS_processing_functions as geostats\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d/%m/%Y %H:%M:%S', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction calcul et mise à jour RScores : adaptée à données globales (SAT, GEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateRScore(ref_df, obs_df):\n",
    "    '''\n",
    "    Estimating Pearson correlation (Rscore) between Observed and Reference time series.\n",
    "    \n",
    "    Note: Rscore is the maximum score obtained from lag cross correlation varying\n",
    "          between -4 and +4 months\n",
    "    '''\n",
    "    \n",
    "    lagmax = 4\n",
    "    date_start_ref = min(ref_df['DATE'])\n",
    "    date_end_ref = max(ref_df['DATE'])\n",
    "    date_start_obs = min(obs_df['DATE'])\n",
    "    date_end_obs = max(obs_df['DATE'])\n",
    "    date_start = max([date_start_ref,date_start_obs])\n",
    "    date_end = min([date_end_ref,date_end_obs])\n",
    "\n",
    "    obs_df = obs_df.drop(columns=['LOCATION'])\n",
    "    ref_df = ref_df.drop(columns=['LOCATION'])\n",
    "    Nb_expect_months = (date_end.year - date_start.year) * 12 + date_end.month - date_start.month + 1\n",
    "\n",
    "    # --- Extracting data on the same period + Interpolation (if needed) ---\n",
    "    ref_df_filt = ref_df.loc[(ref_df['DATE']>=date_start) & (ref_df['DATE']<=date_end)].reset_index(drop=True)\n",
    "    if len(ref_df_filt)!=Nb_expect_months:\n",
    "        ref_df_rsmpl = ref_df.resample('M', on='DATE').mean()\n",
    "        logging.info('\\nREF RESAMPLING')\n",
    "        date_ref = pd.to_datetime(ref_df_rsmpl.index, format='%Y-%m-%d')\n",
    "        date_ref = date_ref.strftime(\"%Y%m\")\n",
    "        date_ref = [d+'01' for d in date_ref]\n",
    "        date_ref = np.array(date_ref)\n",
    "        ref_df_rsmpl = ref_df_rsmpl.reset_index()\n",
    "        ref_df_rsmpl['DATE'] = pd.to_datetime(date_ref, format='%Y%m%d')\n",
    "        ref_df_filt = ref_df_rsmpl[(ref_df_rsmpl['DATE']>=date_start) & (ref_df_rsmpl['DATE']<=date_end)].reset_index(drop=True)\n",
    "        del ref_df_rsmpl, date_ref\n",
    "    if ref_df_filt.isnull().values.any():\n",
    "        count_nan = ref_df_filt.isnull().values.sum()\n",
    "        if count_nan==len(ref_df_filt):\n",
    "            logging.info('\\nNo ref data !')\n",
    "            rmax=np.nan\n",
    "            lagpeak=np.nan\n",
    "            ppeak=np.nan\n",
    "            qscore_ref=np.nan\n",
    "            qscore_obs=np.nan\n",
    "            rs = np.full((1,2*lagmax+1),np.nan)[0]\n",
    "            ps = np.full((1,2*lagmax+1),np.nan)[0]\n",
    "            return rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps\n",
    "        ref_df_filt = ref_df_filt.interpolate()\n",
    "        if ref_df_filt.isnull().values.any():ref_df_filt=ref_df_filt.fillna(method='backfill')\n",
    "        ref_df_filt = ref_df_filt[(ref_df_filt['DATE']>=date_start) & (ref_df_filt['DATE']<=date_end)].reset_index(drop=True)\n",
    "        logging.info('\\nREF INTERPOLATION : {} NaN'.format(str(count_nan)))\n",
    "    qscore_ref = np.round(np.mean(ref_df_filt['QSCORE']), 2)\n",
    "\n",
    "    obs_df_filt = obs_df.loc[(obs_df['DATE']>=date_start) & (obs_df['DATE']<=date_end)].reset_index(drop=True)\n",
    "    if len(obs_df_filt)!=Nb_expect_months:\n",
    "        obs_df_rsmpl = obs_df.resample('M', on='DATE').mean()\n",
    "        logging.info('\\nobs RESAMPLING')\n",
    "        date_ref = pd.to_datetime(obs_df_rsmpl.index, format='%Y-%m-%d')\n",
    "        date_ref = date_ref.strftime(\"%Y%m\")\n",
    "        date_ref = [d+'01' for d in date_ref]\n",
    "        date_ref = np.array(date_ref)\n",
    "        obs_df_rsmpl = obs_df_rsmpl.reset_index()\n",
    "        obs_df_rsmpl['DATE'] = pd.to_datetime(date_ref, format='%Y%m%d')\n",
    "        obs_df_filt = obs_df_rsmpl[(obs_df_rsmpl['DATE']>=date_start) & (obs_df_rsmpl['DATE']<=date_end)].reset_index(drop=True)\n",
    "        del obs_df_rsmpl, date_ref\n",
    "    if obs_df_filt.isnull().values.any():\n",
    "        count_nan = obs_df_filt.isnull().values.sum()\n",
    "        if count_nan==len(obs_df_filt):\n",
    "            logging.info('\\nNo obs data !')\n",
    "            rmax=np.nan\n",
    "            lagpeak=np.nan\n",
    "            ppeak=np.nan\n",
    "            qscore_ref=np.nan\n",
    "            qscore_obs=np.nan\n",
    "            rs = np.full((1,2*lagmax+1),np.nan)[0]\n",
    "            ps = np.full((1,2*lagmax+1),np.nan)[0]\n",
    "            return rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps\n",
    "        obs_df_filt = obs_df_filt.interpolate()\n",
    "        if obs_df_filt.isnull().values.any():obs_df_filt=obs_df_filt.fillna(method='backfill')\n",
    "        obs_df_filt = obs_df_filt[(obs_df_filt['DATE']>=date_start) & (obs_df_filt['DATE']<=date_end)].reset_index(drop=True)\n",
    "        logging.info('\\nOBS INTERPOLATION : {} NaN'.format(str(count_nan)))\n",
    "    qscore_obs = np.round(np.mean(obs_df_filt['QSCORE']), 2)\n",
    "    \n",
    "    # --- Computing Pearson time lag correlation ---\n",
    "    out_crosscorr = [alertmain.crosscorr_pearson(ref_df_filt['MEAN'] , obs_df_filt['MEAN'], lag) for lag in range(-lagmax,lagmax+1)]\n",
    "    rs = np.round(np.array(out_crosscorr)[:,0], 2)\n",
    "    ps = np.array(out_crosscorr)[:,1]\n",
    "    lagpeak = int(np.argmax(rs) - np.floor(len(rs)/2))\n",
    "    rmax = np.round(np.max(rs), 2)\n",
    "    ppeak = ps[np.argmax(rs)]\n",
    "    \n",
    "    return rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction calcul et mise à jour RScores : adaptée à données globales (SAT, GEE) et in-situ (STATIONS MF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateRScore_insitu(meteo_df, obs_df):\n",
    "    '''\n",
    "    Estimating Pearson correlation (Rscore) between meteo and sat time series.\n",
    "    \n",
    "    Note: Rscore is the maximum score obtained from lag cross correlation varying\n",
    "          between -4 and +4 months\n",
    "    '''\n",
    "    \n",
    "    lagmax = 4\n",
    "    date_start_ref = min(meteo_df['DATE'])\n",
    "    date_end_ref = max(meteo_df['DATE'])\n",
    "    date_start_obs = min(obs_df['DATE'])\n",
    "    date_end_obs = max(obs_df['DATE'])\n",
    "    date_start = max([date_start_ref,date_start_obs])\n",
    "    date_end = min([date_end_ref,date_end_obs])\n",
    "\n",
    "    obs_df = obs_df.drop(columns=['LOCATION'])\n",
    "    Nb_expect_months = (date_end.year - date_start.year) * 12 + date_end.month - date_start.month + 1\n",
    "    \n",
    "    # --- Extracting data on the same period + Interpolation (if needed) ---\n",
    "    \n",
    "    # Meteo time series\n",
    "    ind_end = np.where(meteo_df['DATE'] == date_end)\n",
    "    if ind_end[0].size==0:\n",
    "        logging.critical('\\nMeteo data is not available for the last month')\n",
    "        raise Exception('Meteo data is not available for the last month')\n",
    "    meteo_df_val = meteo_df[(meteo_df['DATE']>=date_start) & (meteo_df['DATE']<=date_end)].reset_index(drop=True)\n",
    "    if meteo_df_val.iloc[:,1].isnull().values.any():\n",
    "        count_nan = meteo_df_val.iloc[:,1].isnull().sum()\n",
    "        qscore_meteo = 1 - (count_nan/len(meteo_df_val))\n",
    "        meteo_df.iloc[:,1] = meteo_df.iloc[:,1].interpolate()\n",
    "        if meteo_df.iloc[:,1].isnull().values.any():meteo_df.iloc[:,1]=meteo_df.iloc[:,1].fillna(method='backfill')\n",
    "        meteo_df_val = meteo_df[(meteo_df['DATE']>=date_start) & (meteo_df['DATE']<=date_end)].reset_index(drop=True)\n",
    "        # logging.info('\\nMETEO INTERPOLATION : {} NaN'.format(str(count_nan)))\n",
    "    else:\n",
    "        qscore_meteo = 1\n",
    "    \n",
    "    obs_df_filt = obs_df.loc[(obs_df['DATE']>=date_start) & (obs_df['DATE']<=date_end)].reset_index(drop=True)\n",
    "    if len(obs_df_filt)!=Nb_expect_months:\n",
    "        obs_df_rsmpl = obs_df.resample('M', on='DATE').mean()\n",
    "        logging.info('\\nobs RESAMPLING')\n",
    "        date_ref = pd.to_datetime(obs_df_rsmpl.index, format='%Y-%m-%d')\n",
    "        date_ref = date_ref.strftime(\"%Y%m\")\n",
    "        date_ref = [d+'01' for d in date_ref]\n",
    "        date_ref = np.array(date_ref)\n",
    "        obs_df_rsmpl = obs_df_rsmpl.reset_index()\n",
    "        obs_df_rsmpl['DATE'] = pd.to_datetime(date_ref, format='%Y%m%d')\n",
    "        obs_df_filt = obs_df_rsmpl[(obs_df_rsmpl['DATE']>=date_start) & (obs_df_rsmpl['DATE']<=date_end)].reset_index(drop=True)\n",
    "        del obs_df_rsmpl, date_ref\n",
    "    if obs_df_filt.isnull().values.any():\n",
    "        count_nan = obs_df_filt.isnull().values.sum()\n",
    "        if count_nan==len(obs_df_filt):\n",
    "            logging.info('\\nNo obs data !')\n",
    "            rmax=np.nan\n",
    "            lagpeak=np.nan\n",
    "            ppeak=np.nan\n",
    "            qscore_ref=np.nan\n",
    "            qscore_obs=np.nan\n",
    "            rs = np.full((1,2*lagmax+1),np.nan)[0]\n",
    "            ps = np.full((1,2*lagmax+1),np.nan)[0]\n",
    "            return rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps\n",
    "        obs_df_filt = obs_df_filt.interpolate()\n",
    "        if obs_df_filt.isnull().values.any():obs_df_filt=obs_df_filt.fillna(method='backfill')\n",
    "        obs_df_filt = obs_df_filt[(obs_df_filt['DATE']>=date_start) & (obs_df_filt['DATE']<=date_end)].reset_index(drop=True)\n",
    "        logging.info('\\nOBS INTERPOLATION : {} NaN'.format(str(count_nan)))\n",
    "    qscore_obs = np.round(np.mean(obs_df_filt['QSCORE']), 2)\n",
    "    \n",
    "    # --- Computing Pearson time lag correlation ---\n",
    "    out_crosscorr = [alertmain.crosscorr_pearson(meteo_df_val.iloc[:,1], obs_df_filt['MEAN'], lag) for lag in range(-lagmax,lagmax+1)]\n",
    "    rs = np.round(np.array(out_crosscorr)[:,0], 2)\n",
    "    ps = np.array(out_crosscorr)[:,1]\n",
    "    lagpeak = int(np.argmax(rs) - np.floor(len(rs)/2))\n",
    "    rmax = np.round(np.max(rs), 2)\n",
    "    ppeak = ps[np.argmax(rs)]\n",
    "    # logging.info(f\"rmax={rmax:.2f} at {lagpeak} month(s) for p<{ppeak}\\n\")\n",
    "    \n",
    "    return rmax, lagpeak, ppeak, qscore_meteo, qscore_obs, rs, ps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul Lagged Cross-Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture et préparation de la dataframe OBS\n",
    "\n",
    "instats_obs_csv = glob.glob(os.path.join(INDIR_STATS_OBS, f'*STATS*{PERIOD_OBS}*{LANDMASK_OBS}*.csv'))[0]\n",
    "instats_obs_df = pd.read_csv(instats_obs_csv,sep=';',decimal='.')\n",
    "try:\n",
    "    instats_obs_df['DATE'] = pd.to_datetime(instats_obs_df.DATE, format='%Y-%m-%d')\n",
    "except ValueError:\n",
    "    instats_obs_df['DATE'] = pd.to_datetime(instats_obs_df.DATE, format='%d/%m/%Y')\n",
    "instats_obs_df = instats_obs_df[['LOCATION','DATE','MEAN','QSCORE']].copy()\n",
    "\n",
    "\n",
    "# Lecture et préparation de la dataframe REF\n",
    "\n",
    "if 'SPI_ref_1991_2020' in PRODUCT_REF:\n",
    "    insitu_key=1\n",
    "    instats_ref_csv = glob.glob(os.path.join(INDIR_STATS_REF, f'*{PRODUCT_REF}*.csv'))[0]\n",
    "    instats_ref_df = pd.read_csv(instats_ref_csv,sep=';',decimal=',')\n",
    "    instats_ref_df = instats_ref_df[['NOM','DATE','SPI3_MENS']].copy()\n",
    "    instats_ref_df['DATE'] = pd.to_datetime(instats_ref_df.DATE, format='%Y%m')\n",
    "    stations_spi_csv = os.path.join(ANNEX_DIR, 'Stations', 'SPI_communes_stations.csv')\n",
    "    stations_spi_df = pd.read_csv(stations_spi_csv,sep=';')\n",
    "\n",
    "else:\n",
    "    insitu_key=0\n",
    "    instats_ref_csv = glob.glob(os.path.join(INDIR_STATS_REF, f'*STATS*{PERIOD_REF}*{LANDMASK_REF}*.csv'))[0]\n",
    "    instats_ref_df = pd.read_csv(instats_ref_csv,sep=';',decimal='.')\n",
    "    instats_ref_df = instats_ref_df[['LOCATION','DATE','MEAN','QSCORE']].copy()\n",
    "    try:\n",
    "        instats_ref_df['DATE'] = pd.to_datetime(instats_ref_df.DATE, format='%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        instats_ref_df['DATE'] = pd.to_datetime(instats_ref_df.DATE, format='%d/%m/%Y')\n",
    "\n",
    "area_lut = pd.read_csv(glob.glob(os.path.join(INDIR_STATS_OBS,'ID_Name_Areas-lookup*.csv'))[0], sep=';')\n",
    "# rscore_df = pd.DataFrame(columns=['LOCATION','RMAX','LAGMAX','PVMAX',f'QSCORE {PRODUCT_REF}',f'QSCORE {PRODUCT_OBS}'])\n",
    "rscore_df = pd.DataFrame(columns=['LOCATION','RMAX','LAGMAX','PVMAX',f'QSCORE {PRODUCT_REF}',f'QSCORE {PRODUCT_OBS}',\n",
    "                                  'R-4','R-3','R-2','R-1','R0','R+1','R+2','R+3','R+4',\n",
    "                                  'PV-4','PV-3','PV-2','PV-1','PV0','PV+1','PV+2','PV+3','PV+4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\MATHIS\\0_Projet_Secheresse\\1_Scripts\\toolbox\\eo4dm-oeil\\EO4DM\\..\\DMPIPELINE_WORKST/DATA_HISTO_backup\\New_Caledonia_Fr\\1_INDICATEURS/GLOBAL/STATS\\VHI_STATS_M_NoTrees_NoBuild.csv\n",
      "D:\\MATHIS\\0_Projet_Secheresse\\1_Scripts\\toolbox\\eo4dm-oeil\\EO4DM\\..\\DMPIPELINE_WORKST/DATA_HISTO_backup\\New_Caledonia_Fr\\1_INDICATEURS/ALERT/METEO\\SPI_ref_1991_2020.csv\n"
     ]
    }
   ],
   "source": [
    "print(instats_obs_csv)\n",
    "print(instats_ref_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         LOCATION       DATE  MEAN  QSCORE\n",
      "0           BELEP 2001-01-01  0.61    0.12\n",
      "1     BOULOUPARIS 2001-01-01  0.46    0.20\n",
      "2         BOURAIL 2001-01-01  0.43    0.19\n",
      "3          CANALA 2001-01-01  0.69    0.17\n",
      "4          DUMBEA 2001-01-01  0.45    0.13\n",
      "...           ...        ...   ...     ...\n",
      "9787     SARRAMEA 2024-01-01  0.67    0.13\n",
      "9788         THIO 2024-01-01  0.69    0.16\n",
      "9789        TOUHO 2024-01-01  0.80    0.12\n",
      "9790          VOH 2024-01-01  0.64    0.14\n",
      "9791         YATE 2024-01-01  0.66    0.10\n",
      "\n",
      "[9792 rows x 4 columns]\n",
      "               NOM       DATE  SPI3_MENS\n",
      "0      BOULOUPARIS 1956-01-01        NaN\n",
      "1      BOULOUPARIS 1956-02-01        NaN\n",
      "2      BOULOUPARIS 1956-03-01       1.13\n",
      "3      BOULOUPARIS 1956-04-01       1.23\n",
      "4      BOULOUPARIS 1956-05-01       1.11\n",
      "...            ...        ...        ...\n",
      "35111       OUINNE 2023-09-01       1.00\n",
      "35112       OUINNE 2023-10-01       1.83\n",
      "35113       OUINNE 2023-11-01       1.25\n",
      "35114       OUINNE 2023-12-01      -0.34\n",
      "35115       OUINNE 2024-01-01      -0.46\n",
      "\n",
      "[35116 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(instats_obs_df)\n",
    "print(instats_ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (Optional) Used to clean duplicates dates in dataframes\n",
    "\n",
    "# instats_ref_cleaned = pd.DataFrame(columns=list(instats_ref_df))\n",
    "\n",
    "# for a in tqdm(area_lut['nom'], desc='SUB-AREA'):\n",
    "#     instats_ref_a = instats_ref_df.loc[instats_ref_df['LOCATION']==str(a)].reset_index(drop=True)\n",
    "#     instats_ref_a_cleaned = instats_ref_a.drop_duplicates(subset='DATE')\n",
    "#     instats_ref_cleaned = pd.concat([instats_ref_cleaned, instats_ref_a_cleaned], ignore_index=True)\n",
    "#     del instats_ref_a, instats_ref_a_cleaned\n",
    "\n",
    "# instats_ref_cleaned = instats_ref_cleaned.sort_values(by=['LOCATION','DATE'])\n",
    "# instats_ref_cleaned.to_csv(\n",
    "#   os.path.join(WRK_DIR, os.path.basename(instats_ref_csv)),\n",
    "#   index = False,\n",
    "#   float_format='%.2f',\n",
    "#   decimal = '.',\n",
    "#   sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:   0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : BELEP\n",
      "SUB-AREA : BOULOUPARIS\n",
      "SUB-AREA : BOURAIL\n",
      "SUB-AREA : CANALA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  12%|█▏        | 4/33 [00:00<00:00, 31.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : DUMBEA\n",
      "SUB-AREA : FARINO\n",
      "SUB-AREA : HIENGHENE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  24%|██▍       | 8/33 [00:00<00:00, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : HOUAILOU\n",
      "SUB-AREA : ILE DES PINS\n",
      "SUB-AREA : KAALA GOMEN\n",
      "SUB-AREA : KONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  33%|███▎      | 11/33 [00:00<00:00, 27.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : KOUAOUA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  45%|████▌     | 15/33 [00:00<00:00, 27.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : KOUMAC\n",
      "SUB-AREA : LA FOA\n",
      "SUB-AREA : LIFOU\n",
      "SUB-AREA : MARE\n",
      "SUB-AREA : MOINDOU\n",
      "SUB-AREA : MONT DORE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  55%|█████▍    | 18/33 [00:00<00:00, 27.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : NOUMEA\n",
      "SUB-AREA : OUEGOA\n",
      "SUB-AREA : OUVEA\n",
      "SUB-AREA : PAITA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  67%|██████▋   | 22/33 [00:00<00:00, 28.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : POINDIMIE\n",
      "SUB-AREA : PONERIHOUEN\n",
      "SUB-AREA : POUEBO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  79%|███████▉  | 26/33 [00:00<00:00, 29.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : POUEMBOUT\n",
      "SUB-AREA : POUM\n",
      "SUB-AREA : POYA\n",
      "SUB-AREA : SARRAMEA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA:  91%|█████████ | 30/33 [00:01<00:00, 30.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : THIO\n",
      "SUB-AREA : TOUHO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SUB-AREA: 100%|██████████| 33/33 [00:01<00:00, 29.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-AREA : VOH\n",
      "SUB-AREA : YATE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for a in tqdm(area_lut['nom'], desc='SUB-AREA'):\n",
    "  \n",
    "  print(f'SUB-AREA : {str(a)}')\n",
    "              \n",
    "  # Extract time series on Sub-area\n",
    "  instats_obs_a = instats_obs_df.loc[instats_obs_df['LOCATION']==str(a)].reset_index(drop=True)\n",
    "  testnan_obs_a = instats_obs_a['MEAN']\n",
    "  if insitu_key==1:\n",
    "    instats_ref_a = alertmain.aggregStation(instats_ref_df, stations_spi_df, a)\n",
    "    if instats_ref_a.empty: testnan_ref_a = instats_ref_a\n",
    "    else: testnan_ref_a = instats_ref_a.iloc[:,1]\n",
    "  else:\n",
    "     instats_ref_a = instats_ref_df.loc[instats_ref_df['LOCATION']==str(a)].reset_index(drop=True)\n",
    "     testnan_ref_a = instats_ref_a['MEAN']\n",
    "\n",
    "  # Fill rscore_df with nan if area without data\n",
    "  if (testnan_ref_a.empty or testnan_ref_a.isnull().values.sum()==len(testnan_ref_a)\n",
    "      or testnan_obs_a.empty or testnan_obs_a.isnull().values.sum()==len(testnan_obs_a)):\n",
    "      r_dict = {'LOCATION':str(a),'RMAX':np.nan,'LAGMAX':np.nan,'PVMAX':np.nan,\n",
    "                f'QSCORE {PRODUCT_REF}':np.nan,f'QSCORE {PRODUCT_OBS}':np.nan,\n",
    "                'R-4':np.nan,'R-3':np.nan,'R-2':np.nan,'R-1':np.nan,'R0':np.nan,'R+1':np.nan,'R+2':np.nan,'R+3':np.nan,'R+4':np.nan,\n",
    "                'PV-4':np.nan,'PV-3':np.nan,'PV-2':np.nan,'PV-1':np.nan,'PV0':np.nan,'PV+1':np.nan,'PV+2':np.nan,'PV+3':np.nan,'PV+4':np.nan}\n",
    "  \n",
    "  # Compute Pearson correlation (RSCORE)\n",
    "  else:\n",
    "      instats_obs_a = instats_obs_a.sort_values(by=['DATE'])\n",
    "      instats_ref_a = instats_ref_a.sort_values(by=['DATE'])\n",
    "      if insitu_key==1:\n",
    "         rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps = updateRScore_insitu(instats_ref_a, instats_obs_a)\n",
    "      else:\n",
    "         rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps = updateRScore(instats_ref_a, instats_obs_a)\n",
    "      r_dict = {'LOCATION':str(a),'RMAX':rmax,'LAGMAX':lagpeak,'PVMAX':ppeak,\n",
    "                f'QSCORE {PRODUCT_REF}':qscore_ref,f'QSCORE {PRODUCT_OBS}':qscore_obs,\n",
    "                'R-4':rs[0],'R-3':rs[1],'R-2':rs[2],'R-1':rs[3],'R0':rs[4],'R+1':rs[5],'R+2':rs[6],'R+3':rs[7],'R+4':rs[8],\n",
    "                'PV-4':ps[0],'PV-3':ps[1],'PV-2':ps[2],'PV-1':ps[3],'PV0':ps[4],'PV+1':ps[5],'PV+2':ps[6],'PV+3':ps[7],'PV+4':ps[8]}\n",
    "      del rmax, lagpeak, ppeak, qscore_ref, qscore_obs, rs, ps\n",
    "  \n",
    "  rscore_df = pd.concat([rscore_df, pd.DataFrame([r_dict])], ignore_index=True)\n",
    "  \n",
    "  del r_dict, instats_obs_a, instats_ref_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prépare export rscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = os.path.join(WRK_DIR, f'RUN_RSCORES_DROUGHT_{TERRITORY_str}')\n",
    "os.umask(0) # used to reset the directories permission\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    os.chmod(outdir, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Rscore data frame\n",
    "\n",
    "if LANDMASK_OBS!='': LANDMASK=f'_{LANDMASK_OBS}'\n",
    "elif LANDMASK_REF!='': LANDMASK=f'_{LANDMASK_REF}'\n",
    "else:LANDMASK=''\n",
    "\n",
    "rscore_filename = f'{PRODUCT_OBS}_{PRODUCT_REF}_RSCORE_QSCORES{LANDMASK}.csv'\n",
    "\n",
    "rscore_df.to_csv(\n",
    "  os.path.join(outdir, rscore_filename),\n",
    "  index = False,\n",
    "  decimal = '.',\n",
    "  sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmpipeline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
